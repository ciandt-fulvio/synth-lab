"""align research_executions with ORM model

Revision ID: c65e0b6ff583
Revises: 001
Create Date: 2026-01-02 15:42:57.167681
"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic
revision: str = 'c65e0b6ff583'
down_revision: Union[str, None] = '001'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade database schema."""
    # ### commands auto generated by Alembic - please adjust! ###

    # CRITICAL: Drop transcripts FK to research_executions.id FIRST
    # This allows us to modify research_executions table structure
    op.drop_constraint('transcripts_research_id_fkey', 'transcripts', type_='foreignkey')

    op.alter_column('analysis_cache', 'data',
               existing_type=postgresql.JSON(astext_type=sa.Text()),
               type_=sa.JSON().with_variant(postgresql.JSONB(astext_type=sa.Text()), 'postgresql'),
               existing_nullable=False)
    op.alter_column('analysis_cache', 'params',
               existing_type=postgresql.JSON(astext_type=sa.Text()),
               type_=sa.JSON().with_variant(postgresql.JSONB(astext_type=sa.Text()), 'postgresql'),
               existing_nullable=True)
    op.alter_column('analysis_runs', 'scenario_id',
               existing_type=sa.VARCHAR(length=50),
               server_default=None,
               existing_nullable=False)
    op.alter_column('analysis_runs', 'config',
               existing_type=postgresql.JSON(astext_type=sa.Text()),
               type_=sa.JSON().with_variant(postgresql.JSONB(astext_type=sa.Text()), 'postgresql'),
               existing_nullable=False)
    op.alter_column('analysis_runs', 'status',
               existing_type=sa.VARCHAR(length=20),
               server_default=None,
               existing_nullable=False)
    op.alter_column('analysis_runs', 'total_synths',
               existing_type=sa.INTEGER(),
               server_default=None,
               existing_nullable=False)
    op.alter_column('analysis_runs', 'aggregated_outcomes',
               existing_type=postgresql.JSON(astext_type=sa.Text()),
               type_=sa.JSON().with_variant(postgresql.JSONB(astext_type=sa.Text()), 'postgresql'),
               existing_nullable=True)
    op.add_column('chart_insights', sa.Column('simulation_id', sa.String(length=50), nullable=False))
    op.add_column('chart_insights', sa.Column('insight_type', sa.String(length=50), nullable=False))
    op.add_column('chart_insights', sa.Column('response_json', sa.JSON().with_variant(postgresql.JSONB(astext_type=sa.Text()), 'postgresql'), nullable=False))
    op.add_column('chart_insights', sa.Column('updated_at', sa.String(length=50), nullable=True))
    op.drop_index(op.f('idx_chart_insights_analysis'), table_name='chart_insights')
    op.create_index('idx_chart_insights_simulation', 'chart_insights', ['simulation_id'], unique=False)
    op.create_index('idx_chart_insights_type', 'chart_insights', ['insight_type'], unique=False)
    op.create_unique_constraint('uq_chart_insights_sim_type', 'chart_insights', ['simulation_id', 'insight_type'])
    op.drop_constraint(op.f('chart_insights_analysis_id_fkey'), 'chart_insights', type_='foreignkey')
    op.drop_column('chart_insights', 'analysis_id')
    op.drop_column('chart_insights', 'chart_type')
    op.drop_column('chart_insights', 'data_summary')
    op.drop_column('chart_insights', 'insight_text')
    op.add_column('experiment_documents', sa.Column('document_type', sa.String(length=50), nullable=False))
    op.add_column('experiment_documents', sa.Column('markdown_content', sa.Text(), nullable=False))
    op.add_column('experiment_documents', sa.Column('metadata', sa.JSON().with_variant(postgresql.JSONB(astext_type=sa.Text()), 'postgresql'), nullable=True))
    op.add_column('experiment_documents', sa.Column('generated_at', sa.String(length=50), nullable=False))
    op.add_column('experiment_documents', sa.Column('model', sa.String(length=50), nullable=False))
    op.add_column('experiment_documents', sa.Column('status', sa.String(length=20), nullable=False))
    op.add_column('experiment_documents', sa.Column('error_message', sa.Text(), nullable=True))
    op.drop_constraint(op.f('uq_experiment_documents_type'), 'experiment_documents', type_='unique')
    op.drop_index(op.f('idx_experiment_documents_type'), table_name='experiment_documents')
    op.create_index('idx_experiment_documents_type', 'experiment_documents', ['document_type'], unique=False)
    op.create_index('idx_experiment_documents_status', 'experiment_documents', ['status'], unique=False)
    op.create_unique_constraint('uq_experiment_documents_exp_type', 'experiment_documents', ['experiment_id', 'document_type'])
    op.drop_column('experiment_documents', 'content')
    op.drop_column('experiment_documents', 'updated_at')
    op.drop_column('experiment_documents', 'metadata_')
    op.drop_column('experiment_documents', 'doc_type')
    op.drop_column('experiment_documents', 'created_at')
    op.drop_column('experiment_documents', 'title')
    op.alter_column('experiments', 'scorecard_data',
               existing_type=postgresql.JSON(astext_type=sa.Text()),
               type_=sa.JSON().with_variant(postgresql.JSONB(astext_type=sa.Text()), 'postgresql'),
               existing_nullable=True)
    op.alter_column('experiments', 'status',
               existing_type=sa.VARCHAR(length=20),
               server_default=None,
               existing_nullable=False)
    op.add_column('explorations', sa.Column('goal', sa.JSON().with_variant(postgresql.JSONB(astext_type=sa.Text()), 'postgresql'), nullable=False))
    op.add_column('explorations', sa.Column('config', sa.JSON().with_variant(postgresql.JSONB(astext_type=sa.Text()), 'postgresql'), nullable=False))
    op.add_column('explorations', sa.Column('current_depth', sa.Integer(), nullable=False))
    op.add_column('explorations', sa.Column('total_nodes', sa.Integer(), nullable=False))
    op.add_column('explorations', sa.Column('total_llm_calls', sa.Integer(), nullable=False))
    op.add_column('explorations', sa.Column('best_success_rate', sa.Float(), nullable=True))
    op.add_column('explorations', sa.Column('started_at', sa.String(length=50), nullable=False))
    op.add_column('explorations', sa.Column('completed_at', sa.String(length=50), nullable=True))
    op.alter_column('explorations', 'status',
               existing_type=sa.VARCHAR(length=20),
               server_default=None,
               type_=sa.String(length=50),
               existing_nullable=False)
    op.create_index('idx_explorations_status', 'explorations', ['status'], unique=False)
    op.drop_column('explorations', 'updated_at')
    op.drop_column('explorations', 'name')
    op.drop_column('explorations', 'created_at')
    op.drop_column('explorations', 'description')
    op.add_column('feature_scorecards', sa.Column('experiment_id', sa.String(length=50), nullable=True))
    op.alter_column('feature_scorecards', 'data',
               existing_type=postgresql.JSON(astext_type=sa.Text()),
               type_=sa.JSON().with_variant(postgresql.JSONB(astext_type=sa.Text()), 'postgresql'),
               nullable=False)
    op.create_index('idx_scorecards_experiment', 'feature_scorecards', ['experiment_id'], unique=False)
    op.create_foreign_key(None, 'feature_scorecards', 'experiments', ['experiment_id'], ['id'], ondelete='SET NULL')
    op.drop_column('feature_scorecards', 'name')
    op.drop_column('feature_scorecards', 'description')
    op.add_column('region_analyses', sa.Column('simulation_id', sa.String(length=50), nullable=False))
    op.add_column('region_analyses', sa.Column('rules', sa.JSON().with_variant(postgresql.JSONB(astext_type=sa.Text()), 'postgresql'), nullable=False))
    op.add_column('region_analyses', sa.Column('rule_text', sa.Text(), nullable=False))
    op.add_column('region_analyses', sa.Column('synth_percentage', sa.Float(), nullable=False))
    op.add_column('region_analyses', sa.Column('did_not_try_rate', sa.Float(), nullable=False))
    op.add_column('region_analyses', sa.Column('failed_rate', sa.Float(), nullable=False))
    op.add_column('region_analyses', sa.Column('failure_delta', sa.Float(), nullable=False))
    op.drop_index(op.f('idx_region_analysis'), table_name='region_analyses')
    op.create_index('idx_regions_simulation', 'region_analyses', ['simulation_id'], unique=False)
    op.drop_constraint(op.f('region_analyses_analysis_id_fkey'), 'region_analyses', type_='foreignkey')
    op.drop_column('region_analyses', 'analysis_id')
    op.drop_column('region_analyses', 'characteristics')
    op.drop_column('region_analyses', 'region_name')
    op.drop_column('region_analyses', 'created_at')
    op.add_column('research_executions', sa.Column('exec_id', sa.String(length=100), nullable=False))
    op.add_column('research_executions', sa.Column('topic_name', sa.String(length=200), nullable=False))
    op.add_column('research_executions', sa.Column('synth_count', sa.Integer(), nullable=False))
    op.add_column('research_executions', sa.Column('successful_count', sa.Integer(), nullable=False))
    op.add_column('research_executions', sa.Column('failed_count', sa.Integer(), nullable=False))
    op.add_column('research_executions', sa.Column('model', sa.String(length=50), nullable=False))
    op.add_column('research_executions', sa.Column('max_turns', sa.Integer(), nullable=False))
    op.add_column('research_executions', sa.Column('additional_context', sa.Text(), nullable=True))
    op.alter_column('research_executions', 'status',
               existing_type=sa.VARCHAR(length=20),
               server_default=None,
               type_=sa.String(length=50),
               existing_nullable=False)
    op.drop_index(op.f('idx_research_experiment'), table_name='research_executions')
    op.drop_index(op.f('idx_research_status'), table_name='research_executions')
    op.create_index('idx_executions_experiment', 'research_executions', ['experiment_id'], unique=False)
    op.create_index('idx_executions_started', 'research_executions', ['started_at'], unique=False, postgresql_ops={'started_at': 'DESC'})
    op.create_index('idx_executions_status', 'research_executions', ['status'], unique=False)
    op.create_index('idx_executions_topic', 'research_executions', ['topic_name'], unique=False)
    op.drop_constraint(op.f('research_executions_experiment_id_fkey'), 'research_executions', type_='foreignkey')
    op.drop_constraint(op.f('research_executions_synth_group_id_fkey'), 'research_executions', type_='foreignkey')
    op.create_foreign_key(None, 'research_executions', 'experiments', ['experiment_id'], ['id'], ondelete='SET NULL')
    op.drop_column('research_executions', 'config')
    op.drop_column('research_executions', 'id')
    op.drop_column('research_executions', 'total_synths')
    op.drop_column('research_executions', 'synth_group_id')
    op.drop_column('research_executions', 'completed_synths')
    # Create primary key on exec_id (after dropping old id column)
    op.create_primary_key('research_executions_pkey', 'research_executions', ['exec_id'])
    op.add_column('scenario_nodes', sa.Column('depth', sa.Integer(), nullable=False))
    op.add_column('scenario_nodes', sa.Column('action_applied', sa.Text(), nullable=True))
    op.add_column('scenario_nodes', sa.Column('action_category', sa.String(length=50), nullable=True))
    op.add_column('scenario_nodes', sa.Column('rationale', sa.Text(), nullable=True))
    op.add_column('scenario_nodes', sa.Column('short_action', sa.String(length=200), nullable=True))
    op.add_column('scenario_nodes', sa.Column('scorecard_params', sa.JSON().with_variant(postgresql.JSONB(astext_type=sa.Text()), 'postgresql'), nullable=False))
    op.add_column('scenario_nodes', sa.Column('simulation_results', sa.JSON().with_variant(postgresql.JSONB(astext_type=sa.Text()), 'postgresql'), nullable=True))
    op.add_column('scenario_nodes', sa.Column('execution_time_seconds', sa.Float(), nullable=True))
    op.add_column('scenario_nodes', sa.Column('node_status', sa.String(length=20), nullable=False))
    op.create_index('idx_scenario_nodes_depth', 'scenario_nodes', ['exploration_id', 'depth'], unique=False)
    op.create_index('idx_scenario_nodes_status', 'scenario_nodes', ['exploration_id', 'node_status'], unique=False)
    op.drop_constraint(op.f('scenario_nodes_parent_id_fkey'), 'scenario_nodes', type_='foreignkey')
    op.create_foreign_key(None, 'scenario_nodes', 'scenario_nodes', ['parent_id'], ['id'], ondelete='SET NULL')
    op.drop_column('scenario_nodes', 'results')
    op.drop_column('scenario_nodes', 'scenario_type')
    op.drop_column('scenario_nodes', 'status')
    op.drop_column('scenario_nodes', 'parameters')
    op.add_column('sensitivity_results', sa.Column('simulation_id', sa.String(length=50), nullable=False))
    op.add_column('sensitivity_results', sa.Column('analyzed_at', sa.String(length=50), nullable=False))
    op.add_column('sensitivity_results', sa.Column('deltas_used', sa.JSON().with_variant(postgresql.JSONB(astext_type=sa.Text()), 'postgresql'), nullable=False))
    op.add_column('sensitivity_results', sa.Column('baseline_success', sa.Float(), nullable=False))
    op.add_column('sensitivity_results', sa.Column('most_sensitive_dimension', sa.String(length=100), nullable=False))
    op.add_column('sensitivity_results', sa.Column('dimensions', sa.JSON().with_variant(postgresql.JSONB(astext_type=sa.Text()), 'postgresql'), nullable=False))
    op.drop_index(op.f('idx_sensitivity_analysis'), table_name='sensitivity_results')
    op.create_index('idx_sensitivity_analyzed_at', 'sensitivity_results', ['analyzed_at'], unique=False)
    op.create_index('idx_sensitivity_simulation', 'sensitivity_results', ['simulation_id'], unique=False)
    op.drop_constraint(op.f('sensitivity_results_analysis_id_fkey'), 'sensitivity_results', type_='foreignkey')
    op.drop_column('sensitivity_results', 'feature_name')
    op.drop_column('sensitivity_results', 'analysis_id')
    op.drop_column('sensitivity_results', 'details')
    op.drop_column('sensitivity_results', 'impact_direction')
    op.drop_column('sensitivity_results', 'sensitivity_score')
    op.add_column('simulation_runs', sa.Column('scenario_id', sa.String(length=50), nullable=False))
    op.add_column('simulation_runs', sa.Column('total_synths', sa.Integer(), nullable=False))
    op.add_column('simulation_runs', sa.Column('aggregated_outcomes', sa.JSON().with_variant(postgresql.JSONB(astext_type=sa.Text()), 'postgresql'), nullable=True))
    op.add_column('simulation_runs', sa.Column('execution_time_seconds', sa.Float(), nullable=True))
    op.alter_column('simulation_runs', 'scorecard_id',
               existing_type=sa.VARCHAR(length=50),
               nullable=False)
    op.alter_column('simulation_runs', 'config',
               existing_type=postgresql.JSON(astext_type=sa.Text()),
               type_=sa.JSON().with_variant(postgresql.JSONB(astext_type=sa.Text()), 'postgresql'),
               nullable=False)
    op.alter_column('simulation_runs', 'status',
               existing_type=sa.VARCHAR(length=20),
               server_default=None,
               existing_nullable=False)
    op.drop_index(op.f('idx_simulation_scorecard'), table_name='simulation_runs')
    op.create_index('idx_simulations_scorecard', 'simulation_runs', ['scorecard_id'], unique=False)
    op.create_index('idx_simulations_status', 'simulation_runs', ['status'], unique=False)
    op.drop_column('simulation_runs', 'results')
    op.alter_column('synth_outcomes', 'synth_attributes',
               existing_type=postgresql.JSON(astext_type=sa.Text()),
               type_=sa.JSON().with_variant(postgresql.JSONB(astext_type=sa.Text()), 'postgresql'),
               existing_nullable=True)
    op.alter_column('synths', 'version',
               existing_type=sa.VARCHAR(length=20),
               server_default=None,
               existing_nullable=False)
    op.alter_column('synths', 'data',
               existing_type=postgresql.JSON(astext_type=sa.Text()),
               type_=sa.JSON().with_variant(postgresql.JSONB(astext_type=sa.Text()), 'postgresql'),
               existing_nullable=True)
    op.add_column('transcripts', sa.Column('exec_id', sa.String(length=100), nullable=False))
    op.add_column('transcripts', sa.Column('synth_name', sa.String(length=200), nullable=False))
    op.add_column('transcripts', sa.Column('status', sa.String(length=20), nullable=False))
    op.add_column('transcripts', sa.Column('turn_count', sa.Integer(), nullable=False))
    op.add_column('transcripts', sa.Column('timestamp', sa.String(length=50), nullable=False))
    op.add_column('transcripts', sa.Column('messages', sa.JSON().with_variant(postgresql.JSONB(astext_type=sa.Text()), 'postgresql'), nullable=True))
    op.alter_column('transcripts', 'id',
               existing_type=sa.VARCHAR(length=50),
               type_=sa.String(length=100),
               existing_nullable=False)
    op.drop_index(op.f('idx_transcripts_research'), table_name='transcripts')
    op.drop_constraint(op.f('uq_transcripts_research_synth'), 'transcripts', type_='unique')
    op.create_index('idx_transcripts_exec', 'transcripts', ['exec_id'], unique=False)
    op.create_index('idx_transcripts_synth', 'transcripts', ['synth_id'], unique=False)
    op.create_unique_constraint('uq_transcripts_exec_synth', 'transcripts', ['exec_id', 'synth_id'])
    # FK already dropped at the beginning of upgrade()
    op.create_foreign_key(None, 'transcripts', 'research_executions', ['exec_id'], ['exec_id'], ondelete='CASCADE')
    op.drop_column('transcripts', 'content')
    op.drop_column('transcripts', 'metadata_')
    op.drop_column('transcripts', 'created_at')
    op.drop_column('transcripts', 'research_id')
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade database schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('transcripts', sa.Column('research_id', sa.VARCHAR(length=50), autoincrement=False, nullable=False))
    op.add_column('transcripts', sa.Column('created_at', sa.VARCHAR(length=50), autoincrement=False, nullable=False))
    op.add_column('transcripts', sa.Column('metadata_', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('transcripts', sa.Column('content', sa.TEXT(), autoincrement=False, nullable=True))
    op.drop_constraint(None, 'transcripts', type_='foreignkey')
    op.create_foreign_key(op.f('transcripts_research_id_fkey'), 'transcripts', 'research_executions', ['research_id'], ['id'], ondelete='CASCADE')
    op.drop_constraint('uq_transcripts_exec_synth', 'transcripts', type_='unique')
    op.drop_index('idx_transcripts_synth', table_name='transcripts')
    op.drop_index('idx_transcripts_exec', table_name='transcripts')
    op.create_unique_constraint(op.f('uq_transcripts_research_synth'), 'transcripts', ['research_id', 'synth_id'], postgresql_nulls_not_distinct=False)
    op.create_index(op.f('idx_transcripts_research'), 'transcripts', ['research_id'], unique=False)
    op.alter_column('transcripts', 'id',
               existing_type=sa.String(length=100),
               type_=sa.VARCHAR(length=50),
               existing_nullable=False)
    op.drop_column('transcripts', 'messages')
    op.drop_column('transcripts', 'timestamp')
    op.drop_column('transcripts', 'turn_count')
    op.drop_column('transcripts', 'status')
    op.drop_column('transcripts', 'synth_name')
    op.drop_column('transcripts', 'exec_id')
    op.alter_column('synths', 'data',
               existing_type=sa.JSON().with_variant(postgresql.JSONB(astext_type=sa.Text()), 'postgresql'),
               type_=postgresql.JSON(astext_type=sa.Text()),
               existing_nullable=True)
    op.alter_column('synths', 'version',
               existing_type=sa.VARCHAR(length=20),
               server_default=sa.text("'2.0.0'::character varying"),
               existing_nullable=False)
    op.alter_column('synth_outcomes', 'synth_attributes',
               existing_type=sa.JSON().with_variant(postgresql.JSONB(astext_type=sa.Text()), 'postgresql'),
               type_=postgresql.JSON(astext_type=sa.Text()),
               existing_nullable=True)
    op.add_column('simulation_runs', sa.Column('results', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.drop_index('idx_simulations_status', table_name='simulation_runs')
    op.drop_index('idx_simulations_scorecard', table_name='simulation_runs')
    op.create_index(op.f('idx_simulation_scorecard'), 'simulation_runs', ['scorecard_id'], unique=False)
    op.alter_column('simulation_runs', 'status',
               existing_type=sa.VARCHAR(length=20),
               server_default=sa.text("'pending'::character varying"),
               existing_nullable=False)
    op.alter_column('simulation_runs', 'config',
               existing_type=sa.JSON().with_variant(postgresql.JSONB(astext_type=sa.Text()), 'postgresql'),
               type_=postgresql.JSON(astext_type=sa.Text()),
               nullable=True)
    op.alter_column('simulation_runs', 'scorecard_id',
               existing_type=sa.VARCHAR(length=50),
               nullable=True)
    op.drop_column('simulation_runs', 'execution_time_seconds')
    op.drop_column('simulation_runs', 'aggregated_outcomes')
    op.drop_column('simulation_runs', 'total_synths')
    op.drop_column('simulation_runs', 'scenario_id')
    op.add_column('sensitivity_results', sa.Column('sensitivity_score', sa.DOUBLE_PRECISION(precision=53), autoincrement=False, nullable=False))
    op.add_column('sensitivity_results', sa.Column('impact_direction', sa.VARCHAR(length=20), autoincrement=False, nullable=True))
    op.add_column('sensitivity_results', sa.Column('details', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('sensitivity_results', sa.Column('analysis_id', sa.VARCHAR(length=50), autoincrement=False, nullable=False))
    op.add_column('sensitivity_results', sa.Column('feature_name', sa.VARCHAR(length=100), autoincrement=False, nullable=False))
    op.create_foreign_key(op.f('sensitivity_results_analysis_id_fkey'), 'sensitivity_results', 'analysis_runs', ['analysis_id'], ['id'], ondelete='CASCADE')
    op.drop_index('idx_sensitivity_simulation', table_name='sensitivity_results')
    op.drop_index('idx_sensitivity_analyzed_at', table_name='sensitivity_results')
    op.create_index(op.f('idx_sensitivity_analysis'), 'sensitivity_results', ['analysis_id'], unique=False)
    op.drop_column('sensitivity_results', 'dimensions')
    op.drop_column('sensitivity_results', 'most_sensitive_dimension')
    op.drop_column('sensitivity_results', 'baseline_success')
    op.drop_column('sensitivity_results', 'deltas_used')
    op.drop_column('sensitivity_results', 'analyzed_at')
    op.drop_column('sensitivity_results', 'simulation_id')
    op.add_column('scenario_nodes', sa.Column('parameters', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('scenario_nodes', sa.Column('status', sa.VARCHAR(length=20), server_default=sa.text("'pending'::character varying"), autoincrement=False, nullable=False))
    op.add_column('scenario_nodes', sa.Column('scenario_type', sa.VARCHAR(length=50), autoincrement=False, nullable=False))
    op.add_column('scenario_nodes', sa.Column('results', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.drop_constraint(None, 'scenario_nodes', type_='foreignkey')
    op.create_foreign_key(op.f('scenario_nodes_parent_id_fkey'), 'scenario_nodes', 'scenario_nodes', ['parent_id'], ['id'], ondelete='CASCADE')
    op.drop_index('idx_scenario_nodes_status', table_name='scenario_nodes')
    op.drop_index('idx_scenario_nodes_depth', table_name='scenario_nodes')
    op.drop_column('scenario_nodes', 'node_status')
    op.drop_column('scenario_nodes', 'execution_time_seconds')
    op.drop_column('scenario_nodes', 'simulation_results')
    op.drop_column('scenario_nodes', 'scorecard_params')
    op.drop_column('scenario_nodes', 'short_action')
    op.drop_column('scenario_nodes', 'rationale')
    op.drop_column('scenario_nodes', 'action_category')
    op.drop_column('scenario_nodes', 'action_applied')
    op.drop_column('scenario_nodes', 'depth')
    op.add_column('research_executions', sa.Column('completed_synths', sa.INTEGER(), server_default=sa.text('0'), autoincrement=False, nullable=False))
    op.add_column('research_executions', sa.Column('synth_group_id', sa.VARCHAR(length=50), autoincrement=False, nullable=True))
    op.add_column('research_executions', sa.Column('total_synths', sa.INTEGER(), server_default=sa.text('0'), autoincrement=False, nullable=False))
    op.add_column('research_executions', sa.Column('id', sa.VARCHAR(length=50), autoincrement=False, nullable=False))
    op.add_column('research_executions', sa.Column('config', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.drop_constraint(None, 'research_executions', type_='foreignkey')
    op.create_foreign_key(op.f('research_executions_synth_group_id_fkey'), 'research_executions', 'synth_groups', ['synth_group_id'], ['id'], ondelete='SET NULL')
    op.create_foreign_key(op.f('research_executions_experiment_id_fkey'), 'research_executions', 'experiments', ['experiment_id'], ['id'], ondelete='CASCADE')
    op.drop_index('idx_executions_topic', table_name='research_executions')
    op.drop_index('idx_executions_status', table_name='research_executions')
    op.drop_index('idx_executions_started', table_name='research_executions', postgresql_ops={'started_at': 'DESC'})
    op.drop_index('idx_executions_experiment', table_name='research_executions')
    op.create_index(op.f('idx_research_status'), 'research_executions', ['status'], unique=False)
    op.create_index(op.f('idx_research_experiment'), 'research_executions', ['experiment_id'], unique=False)
    op.alter_column('research_executions', 'status',
               existing_type=sa.String(length=50),
               server_default=sa.text("'pending'::character varying"),
               type_=sa.VARCHAR(length=20),
               existing_nullable=False)
    op.drop_column('research_executions', 'additional_context')
    op.drop_column('research_executions', 'max_turns')
    op.drop_column('research_executions', 'model')
    op.drop_column('research_executions', 'failed_count')
    op.drop_column('research_executions', 'successful_count')
    op.drop_column('research_executions', 'synth_count')
    op.drop_column('research_executions', 'topic_name')
    op.drop_column('research_executions', 'exec_id')
    op.add_column('region_analyses', sa.Column('created_at', sa.VARCHAR(length=50), autoincrement=False, nullable=False))
    op.add_column('region_analyses', sa.Column('region_name', sa.VARCHAR(length=100), autoincrement=False, nullable=False))
    op.add_column('region_analyses', sa.Column('characteristics', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('region_analyses', sa.Column('analysis_id', sa.VARCHAR(length=50), autoincrement=False, nullable=False))
    op.create_foreign_key(op.f('region_analyses_analysis_id_fkey'), 'region_analyses', 'analysis_runs', ['analysis_id'], ['id'], ondelete='CASCADE')
    op.drop_index('idx_regions_simulation', table_name='region_analyses')
    op.create_index(op.f('idx_region_analysis'), 'region_analyses', ['analysis_id'], unique=False)
    op.drop_column('region_analyses', 'failure_delta')
    op.drop_column('region_analyses', 'failed_rate')
    op.drop_column('region_analyses', 'did_not_try_rate')
    op.drop_column('region_analyses', 'synth_percentage')
    op.drop_column('region_analyses', 'rule_text')
    op.drop_column('region_analyses', 'rules')
    op.drop_column('region_analyses', 'simulation_id')
    op.add_column('feature_scorecards', sa.Column('description', sa.TEXT(), autoincrement=False, nullable=True))
    op.add_column('feature_scorecards', sa.Column('name', sa.VARCHAR(length=200), autoincrement=False, nullable=False))
    op.drop_constraint(None, 'feature_scorecards', type_='foreignkey')
    op.drop_index('idx_scorecards_experiment', table_name='feature_scorecards')
    op.alter_column('feature_scorecards', 'data',
               existing_type=sa.JSON().with_variant(postgresql.JSONB(astext_type=sa.Text()), 'postgresql'),
               type_=postgresql.JSON(astext_type=sa.Text()),
               nullable=True)
    op.drop_column('feature_scorecards', 'experiment_id')
    op.add_column('explorations', sa.Column('description', sa.TEXT(), autoincrement=False, nullable=True))
    op.add_column('explorations', sa.Column('created_at', sa.VARCHAR(length=50), autoincrement=False, nullable=False))
    op.add_column('explorations', sa.Column('name', sa.VARCHAR(length=200), autoincrement=False, nullable=False))
    op.add_column('explorations', sa.Column('updated_at', sa.VARCHAR(length=50), autoincrement=False, nullable=True))
    op.drop_index('idx_explorations_status', table_name='explorations')
    op.alter_column('explorations', 'status',
               existing_type=sa.String(length=50),
               server_default=sa.text("'active'::character varying"),
               type_=sa.VARCHAR(length=20),
               existing_nullable=False)
    op.drop_column('explorations', 'completed_at')
    op.drop_column('explorations', 'started_at')
    op.drop_column('explorations', 'best_success_rate')
    op.drop_column('explorations', 'total_llm_calls')
    op.drop_column('explorations', 'total_nodes')
    op.drop_column('explorations', 'current_depth')
    op.drop_column('explorations', 'config')
    op.drop_column('explorations', 'goal')
    op.alter_column('experiments', 'status',
               existing_type=sa.VARCHAR(length=20),
               server_default=sa.text("'active'::character varying"),
               existing_nullable=False)
    op.alter_column('experiments', 'scorecard_data',
               existing_type=sa.JSON().with_variant(postgresql.JSONB(astext_type=sa.Text()), 'postgresql'),
               type_=postgresql.JSON(astext_type=sa.Text()),
               existing_nullable=True)
    op.add_column('experiment_documents', sa.Column('title', sa.VARCHAR(length=200), autoincrement=False, nullable=True))
    op.add_column('experiment_documents', sa.Column('created_at', sa.VARCHAR(length=50), autoincrement=False, nullable=False))
    op.add_column('experiment_documents', sa.Column('doc_type', sa.VARCHAR(length=50), autoincrement=False, nullable=False))
    op.add_column('experiment_documents', sa.Column('metadata_', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('experiment_documents', sa.Column('updated_at', sa.VARCHAR(length=50), autoincrement=False, nullable=True))
    op.add_column('experiment_documents', sa.Column('content', sa.TEXT(), autoincrement=False, nullable=True))
    op.drop_constraint('uq_experiment_documents_exp_type', 'experiment_documents', type_='unique')
    op.drop_index('idx_experiment_documents_status', table_name='experiment_documents')
    op.drop_index('idx_experiment_documents_type', table_name='experiment_documents')
    op.create_index(op.f('idx_experiment_documents_type'), 'experiment_documents', ['doc_type'], unique=False)
    op.create_unique_constraint(op.f('uq_experiment_documents_type'), 'experiment_documents', ['experiment_id', 'doc_type'], postgresql_nulls_not_distinct=False)
    op.drop_column('experiment_documents', 'error_message')
    op.drop_column('experiment_documents', 'status')
    op.drop_column('experiment_documents', 'model')
    op.drop_column('experiment_documents', 'generated_at')
    op.drop_column('experiment_documents', 'metadata')
    op.drop_column('experiment_documents', 'markdown_content')
    op.drop_column('experiment_documents', 'document_type')
    op.add_column('chart_insights', sa.Column('insight_text', sa.TEXT(), autoincrement=False, nullable=False))
    op.add_column('chart_insights', sa.Column('data_summary', postgresql.JSON(astext_type=sa.Text()), autoincrement=False, nullable=True))
    op.add_column('chart_insights', sa.Column('chart_type', sa.VARCHAR(length=50), autoincrement=False, nullable=False))
    op.add_column('chart_insights', sa.Column('analysis_id', sa.VARCHAR(length=50), autoincrement=False, nullable=False))
    op.create_foreign_key(op.f('chart_insights_analysis_id_fkey'), 'chart_insights', 'analysis_runs', ['analysis_id'], ['id'], ondelete='CASCADE')
    op.drop_constraint('uq_chart_insights_sim_type', 'chart_insights', type_='unique')
    op.drop_index('idx_chart_insights_type', table_name='chart_insights')
    op.drop_index('idx_chart_insights_simulation', table_name='chart_insights')
    op.create_index(op.f('idx_chart_insights_analysis'), 'chart_insights', ['analysis_id'], unique=False)
    op.drop_column('chart_insights', 'updated_at')
    op.drop_column('chart_insights', 'response_json')
    op.drop_column('chart_insights', 'insight_type')
    op.drop_column('chart_insights', 'simulation_id')
    op.alter_column('analysis_runs', 'aggregated_outcomes',
               existing_type=sa.JSON().with_variant(postgresql.JSONB(astext_type=sa.Text()), 'postgresql'),
               type_=postgresql.JSON(astext_type=sa.Text()),
               existing_nullable=True)
    op.alter_column('analysis_runs', 'total_synths',
               existing_type=sa.INTEGER(),
               server_default=sa.text('0'),
               existing_nullable=False)
    op.alter_column('analysis_runs', 'status',
               existing_type=sa.VARCHAR(length=20),
               server_default=sa.text("'pending'::character varying"),
               existing_nullable=False)
    op.alter_column('analysis_runs', 'config',
               existing_type=sa.JSON().with_variant(postgresql.JSONB(astext_type=sa.Text()), 'postgresql'),
               type_=postgresql.JSON(astext_type=sa.Text()),
               existing_nullable=False)
    op.alter_column('analysis_runs', 'scenario_id',
               existing_type=sa.VARCHAR(length=50),
               server_default=sa.text("'baseline'::character varying"),
               existing_nullable=False)
    op.alter_column('analysis_cache', 'params',
               existing_type=sa.JSON().with_variant(postgresql.JSONB(astext_type=sa.Text()), 'postgresql'),
               type_=postgresql.JSON(astext_type=sa.Text()),
               existing_nullable=True)
    op.alter_column('analysis_cache', 'data',
               existing_type=sa.JSON().with_variant(postgresql.JSONB(astext_type=sa.Text()), 'postgresql'),
               type_=postgresql.JSON(astext_type=sa.Text()),
               existing_nullable=False)
    # ### end Alembic commands ###
