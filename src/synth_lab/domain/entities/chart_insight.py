"""
Chart insight entities for LLM-generated analysis captions and insights.

Provides structured representations for chart captions, full insights,
and executive summaries generated by LLM analysis.

References:
    - Data Model: specs/017-analysis-ux-research/data-model.md
    - Spec: specs/017-analysis-ux-research/spec.md (US6)

Sample Input:
    simulation_id: str, chart_type: str, chart_data: dict

Expected Output:
    ChartInsight: Caption + explanation + evidence + recommendation
"""

from datetime import datetime, timezone
from typing import Literal

from pydantic import BaseModel, Field


# Valid chart types for insights
ChartType = Literal[
    "try_vs_success",
    "distribution",
    "sankey",
    "failure_heatmap",
    "scatter",
    "box_plot",
    "clustering",
    "outliers",
    "shap_summary",
    "pdp",
]


class ChartCaption(BaseModel):
    """
    Short caption generated by LLM for a chart.

    The caption should be factual and <=20 tokens, summarizing
    the key takeaway from the chart.
    """

    simulation_id: str = Field(description="Simulation identifier")
    chart_type: ChartType = Field(description="Type of chart")
    caption: str = Field(description="Short caption (<=20 tokens)")
    key_metric: str = Field(description="Name of the key metric highlighted")
    key_value: float = Field(description="Value of the key metric")
    confidence: float = Field(
        default=1.0, ge=0.0, le=1.0, description="Confidence score (0-1)"
    )


class ChartInsight(BaseModel):
    """
    Full insight generated by LLM for a chart.

    Includes caption, detailed explanation, evidence from data,
    and optional recommendations for action.
    """

    simulation_id: str = Field(description="Simulation identifier")
    chart_type: ChartType = Field(description="Type of chart")
    caption: str = Field(description="Short factual caption (<=20 tokens)")
    explanation: str = Field(description="Detailed explanation (<=200 tokens)")
    evidence: list[str] = Field(
        default_factory=list, description="Data points supporting the insight"
    )
    recommendation: str | None = Field(
        default=None, description="Actionable recommendation based on insight"
    )
    confidence: float = Field(
        default=1.0, ge=0.0, le=1.0, description="Confidence score (0-1)"
    )
    generated_at: str = Field(
        default_factory=lambda: datetime.now(timezone.utc).isoformat(),
        description="ISO timestamp when insight was generated",
    )


class SimulationInsights(BaseModel):
    """
    Collection of all insights for a simulation.

    Contains insights for each chart type plus an optional
    executive summary consolidating all findings.
    """

    simulation_id: str = Field(description="Simulation identifier")
    insights: dict[str, ChartInsight] = Field(
        default_factory=dict, description="Map of chart_type to insight"
    )
    executive_summary: str | None = Field(
        default=None, description="Consolidated executive summary"
    )
    generated_at: str = Field(
        default_factory=lambda: datetime.now(timezone.utc).isoformat(),
        description="ISO timestamp when insights were generated",
    )
    total_charts_analyzed: int = Field(
        default=0, description="Number of charts analyzed"
    )

    @property
    def chart_types_covered(self) -> list[str]:
        """Get list of chart types that have insights."""
        return list(self.insights.keys())


# =============================================================================
# Validation
# =============================================================================

if __name__ == "__main__":
    import sys

    all_validation_failures: list[str] = []
    total_tests = 0

    # Test 1: ChartCaption creation
    total_tests += 1
    try:
        caption = ChartCaption(
            simulation_id="sim_12345678",
            chart_type="try_vs_success",
            caption="42% of synths in usability issue quadrant",
            key_metric="usability_issue_percentage",
            key_value=42.0,
            confidence=0.95,
        )
        if caption.chart_type != "try_vs_success":
            all_validation_failures.append("ChartCaption chart_type mismatch")
        elif caption.confidence != 0.95:
            all_validation_failures.append("ChartCaption confidence mismatch")
        else:
            print("Test 1 PASSED: ChartCaption created correctly")
    except Exception as e:
        all_validation_failures.append(f"ChartCaption creation failed: {e}")

    # Test 2: ChartInsight creation
    total_tests += 1
    try:
        insight = ChartInsight(
            simulation_id="sim_12345678",
            chart_type="clustering",
            caption="3 distinct user segments identified",
            explanation="K-means clustering reveals 3 groups: power users (30%), "
            "strugglers (40%), and casual users (30%). Power users have high "
            "capability and trust, while strugglers have low values for both.",
            evidence=[
                "Silhouette score: 0.72",
                "Cluster 0: 30% of users, 85% success rate",
                "Cluster 1: 40% of users, 15% success rate",
            ],
            recommendation="Focus on converting strugglers through onboarding improvements",
        )
        if insight.chart_type != "clustering":
            all_validation_failures.append("ChartInsight chart_type mismatch")
        elif len(insight.evidence) != 3:
            all_validation_failures.append("ChartInsight evidence count mismatch")
        elif insight.recommendation is None:
            all_validation_failures.append("ChartInsight recommendation should be set")
        else:
            print("Test 2 PASSED: ChartInsight created correctly")
    except Exception as e:
        all_validation_failures.append(f"ChartInsight creation failed: {e}")

    # Test 3: ChartInsight defaults
    total_tests += 1
    try:
        insight = ChartInsight(
            simulation_id="sim_12345678",
            chart_type="sankey",
            caption="30% drop-off before attempt",
            explanation="Significant number of synths never try the feature.",
        )
        if insight.evidence != []:
            all_validation_failures.append("ChartInsight should default to empty evidence")
        elif insight.recommendation is not None:
            all_validation_failures.append("ChartInsight recommendation should be None by default")
        elif insight.generated_at is None:
            all_validation_failures.append("ChartInsight generated_at should be auto-set")
        else:
            print("Test 3 PASSED: ChartInsight defaults work correctly")
    except Exception as e:
        all_validation_failures.append(f"ChartInsight defaults test failed: {e}")

    # Test 4: SimulationInsights creation
    total_tests += 1
    try:
        insights = SimulationInsights(
            simulation_id="sim_12345678",
            insights={
                "try_vs_success": ChartInsight(
                    simulation_id="sim_12345678",
                    chart_type="try_vs_success",
                    caption="Test caption",
                    explanation="Test explanation",
                ),
                "clustering": ChartInsight(
                    simulation_id="sim_12345678",
                    chart_type="clustering",
                    caption="Cluster caption",
                    explanation="Cluster explanation",
                ),
            },
            executive_summary="Overall, the feature shows good adoption.",
            total_charts_analyzed=2,
        )
        if len(insights.insights) != 2:
            all_validation_failures.append("SimulationInsights insights count mismatch")
        elif insights.executive_summary is None:
            all_validation_failures.append("SimulationInsights executive_summary should be set")
        elif insights.chart_types_covered != ["try_vs_success", "clustering"]:
            all_validation_failures.append(
                f"SimulationInsights chart_types_covered mismatch: {insights.chart_types_covered}"
            )
        else:
            print("Test 4 PASSED: SimulationInsights created correctly")
    except Exception as e:
        all_validation_failures.append(f"SimulationInsights creation failed: {e}")

    # Test 5: Invalid chart type
    total_tests += 1
    try:
        ChartCaption(
            simulation_id="sim_12345678",
            chart_type="invalid_type",  # type: ignore
            caption="Test",
            key_metric="test",
            key_value=0.0,
        )
        all_validation_failures.append("Should reject invalid chart_type")
    except Exception:
        print("Test 5 PASSED: Invalid chart_type correctly rejected")

    # Test 6: Confidence bounds
    total_tests += 1
    try:
        ChartInsight(
            simulation_id="sim_12345678",
            chart_type="sankey",
            caption="Test",
            explanation="Test",
            confidence=1.5,  # > 1.0
        )
        all_validation_failures.append("Should reject confidence > 1.0")
    except Exception:
        print("Test 6 PASSED: Invalid confidence correctly rejected")

    # Final result
    print()
    if all_validation_failures:
        failed = len(all_validation_failures)
        print(f"❌ VALIDATION FAILED - {failed} of {total_tests} tests failed:")
        for failure in all_validation_failures:
            print(f"  - {failure}")
        sys.exit(1)
    else:
        print(f"✅ VALIDATION PASSED - All {total_tests} tests produced expected results")
        sys.exit(0)
