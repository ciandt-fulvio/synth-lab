# Quickstart: Mini-Jaeger Local para Conversas LLM Multi-turn

**Feature**: 008-trace-visualizer | **Date**: 2025-12-17 | **Phase**: 1 (Design)

## Vis√£o Geral

Este guia mostra como usar o trace visualizer para registrar e visualizar conversas multi-turn com LLMs.

**O que voc√™ vai fazer**:
1. Instrumentar c√≥digo para registrar traces
2. Executar conversa e gerar arquivo `.trace.json`
3. Visualizar trace no navegador
4. Inspecionar detalhes de etapas (prompts, respostas, tool calls)

**Tempo estimado**: 10 minutos

---

## Instala√ß√£o

O trace visualizer est√° inclu√≠do no synth-lab. Sem depend√™ncias externas.

```bash
# J√° instalado se voc√™ tem synth-lab
uv pip install synth-lab
```

---

## Uso B√°sico: Registrar uma Conversa

### Passo 1: Importar o Tracer

```python
from synth_lab.trace_visualizer import Tracer
```

### Passo 2: Criar Trace e Instrumentar C√≥digo

```python
from synth_lab.trace_visualizer import Tracer, SpanType, SpanStatus

# Criar tracer para conversa
tracer = Tracer(trace_id="conv-weather-demo")

# Iniciar turn (itera√ß√£o da conversa)
with tracer.start_turn(turn_number=1):

    # Registrar LLM call
    with tracer.start_span(SpanType.LLM_CALL, attributes={
        "prompt": "What is the weather in Paris?",
        "model": "claude-sonnet-4-5"
    }) as span:
        # Simular chamada LLM
        response = "Let me check the weather for you."
        span.set_attribute("response", response)
        span.set_attribute("tokens_input", 8)
        span.set_attribute("tokens_output", 10)

    # Registrar tool call
    with tracer.start_span(SpanType.TOOL_CALL, attributes={
        "tool_name": "get_weather",
        "arguments": {"city": "Paris"}
    }) as span:
        # Simular chamada tool
        result = {"temp": 15, "condition": "cloudy"}
        span.set_attribute("result", result)
        span.set_status(SpanStatus.SUCCESS)

    # Registrar resposta final
    with tracer.start_span(SpanType.LLM_CALL, attributes={
        "prompt": "Format weather response",
        "model": "claude-sonnet-4-5"
    }) as span:
        response = "The weather in Paris is 15¬∞C and cloudy."
        span.set_attribute("response", response)

# Salvar trace
tracer.save_trace("data/traces/weather-demo.trace.json")
print("‚úÖ Trace salvo em weather-demo.trace.json")
```

### Passo 3: Visualizar no Navegador

1. Abrir `logui/index.html` no navegador (Chrome, Firefox, Safari)
2. Clicar em "Load Trace" e selecionar `weather-demo.trace.json`
3. Explorar waterfall: ver timeline de execu√ß√£o
4. Clicar em etapas para ver detalhes (prompts, respostas, argumentos)

**Screenshot esperado**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Turn 1                  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚îÇ
‚îÇ   ‚îú‚îÄ LLM Call          ‚ñà‚ñà                       ‚îÇ
‚îÇ   ‚îú‚îÄ Tool Call         ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         ‚îÇ
‚îÇ   ‚îî‚îÄ LLM Call          ‚ñà‚ñà                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Exemplo Completo: Conversa Multi-Turn

```python
from synth_lab.trace_visualizer import Tracer, SpanType, SpanStatus
import time

# Criar tracer com metadata opcional
tracer = Tracer(
    trace_id="multi-turn-weather",
    metadata={"user_id": "demo-user", "session": "example"}
)

# Turn 1: Usu√°rio pergunta sobre Paris
with tracer.start_turn(turn_number=1):
    with tracer.start_span(SpanType.LLM_CALL, attributes={
        "prompt": "What is the weather in Paris?",
        "model": "claude-sonnet-4-5"
    }) as span:
        time.sleep(0.5)  # Simular lat√™ncia LLM
        span.set_attribute("response", "Let me check.")
        span.set_attribute("tokens_input", 8)
        span.set_attribute("tokens_output", 5)

    with tracer.start_span(SpanType.TOOL_CALL, attributes={
        "tool_name": "get_weather",
        "arguments": {"city": "Paris"}
    }) as span:
        time.sleep(1.0)  # Simular lat√™ncia API
        span.set_attribute("result", {"temp": 15, "condition": "cloudy"})
        span.set_status(SpanStatus.SUCCESS)

# Turn 2: Usu√°rio pergunta sobre previs√£o
with tracer.start_turn(turn_number=2):
    with tracer.start_span(SpanType.LLM_CALL, attributes={
        "prompt": "What about tomorrow?",
        "model": "claude-sonnet-4-5"
    }) as span:
        time.sleep(0.5)  # Simular lat√™ncia LLM
        span.set_attribute("response", "I don't have forecast data.")

# Turn 3: Usu√°rio tenta London (com erro)
with tracer.start_turn(turn_number=3):
    with tracer.start_span(SpanType.LLM_CALL, attributes={
        "prompt": "How about London?",
        "model": "claude-sonnet-4-5"
    }) as span:
        time.sleep(0.5)
        span.set_attribute("response", "Let me check London.")

    with tracer.start_span(SpanType.TOOL_CALL, attributes={
        "tool_name": "get_weather",
        "arguments": {"city": "London"}
    }) as span:
        time.sleep(1.0)
        # Simular erro
        span.set_status(SpanStatus.ERROR)
        span.set_attribute("error_message", "API timeout after 1000ms")

    # Registrar erro explicitamente
    with tracer.start_span(SpanType.ERROR, attributes={
        "error_type": "APITimeoutError",
        "error_message": "Weather API did not respond in time"
    }):
        pass

# Salvar e visualizar
tracer.save_trace("data/traces/multi-turn-demo.trace.json")
print(f"‚úÖ Trace salvo: {tracer.trace.duration_ms}ms total")
print(f"   Turns: {len(tracer.trace.turns)}")
print(f"   Total steps: {sum(len(t.steps) for t in tracer.trace.turns)}")
```

**Output esperado**:
```
‚úÖ Trace salvo: 4500ms total
   Turns: 3
   Total steps: 5
```

---

## API Reference R√°pida

### Classe `Tracer`

**Construtor**:
```python
tracer = Tracer(
    trace_id="conversation-id",  # Opcional: auto-gerado se omitido
    metadata={"user_id": "123"}   # Opcional: metadados customizados
)
```

**M√©todos**:

| M√©todo | Descri√ß√£o | Exemplo |
|--------|-----------|---------|
| `start_turn(turn_number)` | Inicia novo turn (context manager) | `with tracer.start_turn(1): ...` |
| `start_span(type, attributes)` | Inicia span (context manager) | `with tracer.start_span("llm_call", {...}): ...` |
| `save_trace(path)` | Salva trace em JSON | `tracer.save_trace("trace.json")` |

### Span Types

| Tipo | Quando Usar | Atributos Obrigat√≥rios |
|------|-------------|------------------------|
| `llm_call` | Chamada LLM (prompt ‚Üí response) | `prompt`, `response`, `model` |
| `tool_call` | Execu√ß√£o de ferramenta | `tool_name`, `arguments` |
| `logic` | L√≥gica de neg√≥cio | `operation` |
| `error` | Opera√ß√£o com erro | `error_type`, `error_message` |

### Span Attributes

**Configurar atributo durante cria√ß√£o**:
```python
with tracer.start_span("llm_call", attributes={"prompt": "..."}) as span:
    pass
```

**Configurar atributo durante execu√ß√£o**:
```python
with tracer.start_span("tool_call", attributes={...}) as span:
    result = call_tool()
    span.set_attribute("result", result)
    span.set_status("success")  # ou "error"
```

---

## Visualiza√ß√£o no Navegador

### Abrir UI

1. Navegar at√© `logui/index.html` no navegador
2. Ou usar servidor local:
   ```bash
   cd ui
   python3 -m http.server 8000
   # Abrir http://localhost:8000
   ```

### Carregar Trace

1. Clicar em "Load Trace" (ou arrastar arquivo `.trace.json`)
2. Trace ser√° renderizado em waterfall

### Navega√ß√£o

- **Waterfall View**: Timeline horizontal mostrando dura√ß√£o de etapas
- **Expand/Collapse**: Clicar em turn para expandir/colapsar steps
- **Detail Panel**: Clicar em step para ver detalhes (sidebar √† direita)
- **Cores**:
  - üîµ **Azul**: LLM calls
  - üü¢ **Verde**: Tool calls
  - üî¥ **Vermelho**: Errors
  - üü° **Amarelo**: Logic/business logic

### Inspecionar Detalhes

Clicar em qualquer step abre painel lateral com:
- **Type**: Tipo da opera√ß√£o
- **Duration**: Tempo de execu√ß√£o
- **Status**: success/error
- **Attributes**: Prompt, response, args, resultado, etc.

**Exemplo - LLM Call**:
```
Type: llm_call
Duration: 3250ms
Status: success

Attributes:
  prompt: "What is the weather in Paris?"
  response: "Let me check the weather for you."
  model: "claude-sonnet-4-5"
```

**Exemplo - Tool Call (Error)**:
```
Type: tool_call
Duration: 1000ms
Status: error

Attributes:
  tool_name: "get_weather"
  arguments: {"city": "London"}
  error_message: "API timeout after 1000ms"
```

---

## Boas Pr√°ticas

### 1. Nomear Traces com Contexto

```python
# ‚úÖ BOM: Identificador claro
tracer = Tracer(trace_id="user-123-weather-query")

# ‚ùå RUIM: Gen√©rico
tracer = Tracer(trace_id="trace-001")
```

### 2. Usar Turns para Itera√ß√µes

```python
# ‚úÖ BOM: Cada pergunta do usu√°rio = novo turn
with tracer.start_turn(turn_number=1):  # Primeira pergunta
    # ...

with tracer.start_turn(turn_number=2):  # Segunda pergunta
    # ...

# ‚ùå RUIM: Tudo no mesmo turn
with tracer.start_turn(turn_number=1):
    # ... m√∫ltiplas intera√ß√µes sem separa√ß√£o
```

### 3. Capturar Erros Explicitamente

```python
# ‚úÖ BOM: Span de erro expl√≠cito
try:
    result = call_api()
except Exception as e:
    with tracer.start_span(
        span_type="error",
        attributes={
            "error_type": type(e).__name__,
            "error_message": str(e)
        }
    ):
        pass

# ‚ùå RUIM: Erro n√£o registrado no trace
try:
    result = call_api()
except Exception:
    pass  # Erro invis√≠vel no trace
```

### 4. Adicionar Contexto em Attributes

```python
# ‚úÖ BOM: Attributes informativos
with tracer.start_span(
    span_type="tool_call",
    attributes={
        "tool_name": "get_weather",
        "arguments": {"city": "Paris", "units": "celsius"},
        "api_endpoint": "https://weather.api/v1/current"
    }
) as span:
    result = get_weather("Paris")
    span.set_attribute("result", result)
    span.set_attribute("api_latency_ms", 1250)

# ‚ùå RUIM: Attributes m√≠nimos
with tracer.start_span("tool_call", attributes={"tool_name": "tool"}):
    get_weather("Paris")
```

### 5. Truncar Prompts Longos (UI Autom√°tico)

UI trunca automaticamente prompts >500 chars (FR-006), mas voc√™ pode pr√©-truncar:

```python
prompt = "..." * 1000  # Muito longo
if len(prompt) > 1000:
    prompt_display = prompt[:1000] + "... [truncated]"
else:
    prompt_display = prompt

with tracer.start_span(
    "llm_call",
    attributes={"prompt": prompt_display}
):
    pass
```

---

## Troubleshooting

### Trace n√£o aparece no UI

**Problema**: Carregou `.trace.json` mas waterfall est√° vazio.

**Solu√ß√£o**:
1. Verificar JSON v√°lido: `python3 -m json.tool trace.json`
2. Verificar estrutura: pelo menos 1 turn, 1 step por turn
3. Abrir console do navegador (F12) para erros JavaScript

### Arquivo muito grande (>5MB)

**Problema**: Trace com muitas etapas causa lentid√£o no navegador.

**Solu√ß√£o**:
1. Dividir conversa em m√∫ltiplos traces menores
2. Limitar turns por trace (recomendado: <20 turns)
3. Truncar prompts/responses longos antes de salvar

### Timestamps incorretos

**Problema**: Dura√ß√£o de steps n√£o bate com expectativa.

**Solu√ß√£o**:
1. Verificar que context managers (`with`) s√£o usados corretamente
2. Verificar que `start_span` √© chamado ANTES da opera√ß√£o
3. Timestamps s√£o em UTC (ISO 8601)

### UI n√£o carrega (CORS error)

**Problema**: `file://` protocol bloqueado pelo navegador.

**Solu√ß√£o**:
```bash
# Usar servidor local
cd ui
python3 -m http.server 8000
# Abrir http://localhost:8000
```

---

## Limita√ß√µes Conhecidas

| Limita√ß√£o | Impacto | Mitiga√ß√£o |
|-----------|---------|-----------|
| M√°x. 100 steps por trace | Performance UI | Dividir conversas longas |
| M√°x. 5MB por arquivo | Navegador pode travar | Truncar prompts/responses |
| Sem agrega√ß√£o estat√≠stica | N√£o v√™ m√©dias/totais | Usar ferramenta externa (futuro) |
| Manual testing only (UI) | Sem testes autom√°ticos | Seguir BDD checklist (spec.md) |

---

## Pr√≥ximos Passos

1. ‚úÖ **Leia**: [data-model.md](data-model.md) para entender estrutura JSON
2. ‚úÖ **Experimente**: Rode exemplo completo acima
3. ‚úÖ **Visualize**: Abra UI e explore trace gerado
4. ‚úÖ **Instrumente**: Adicione tracer ao seu c√≥digo LLM
5. ‚è≥ **Avan√ßado**: Exportar/importar traces (P2 feature)

---

## Recursos Adicionais

- **Especifica√ß√£o**: [spec.md](spec.md) - Requisitos completos
- **Data Model**: [data-model.md](data-model.md) - Estrutura JSON detalhada
- **Implementation Plan**: [plan.md](plan.md) - Detalhes t√©cnicos

---

## Feedback

Encontrou um bug ou tem sugest√£o? Abra issue no reposit√≥rio synth-lab.
